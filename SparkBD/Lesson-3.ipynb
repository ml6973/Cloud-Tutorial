{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border:none;color:#333;background-color:#333;\" />\n",
    "<img style=\" float:right; display:inline\" src=\"http://opencloud.utsa.edu/wp-content/themes/utsa-oci/images/logo.png\"/>\n",
    "\n",
    "### **University of Texas at San Antonio** \n",
    "<br/>\n",
    "<br/>\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 2.5em;\"> **Open Cloud Institute** </span>\n",
    "\n",
    "<hr style=\"height:3px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Lesson 3: Big Data Analysis with Spark\n",
    "\n",
    "<br/>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Nimish Joshi, Research Fellow** </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.4em;\"> *Open Cloud Institute, University of Texas at San Antonio, San Antonio, Texas, USA* </span>  \n",
    "\n",
    "<hr style=\"height:3px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Pi Value Example Using SparkContext** </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> The idea of this lesson is to calculate the pi value using the python code and user imput value that leverages the Spark platform to perform its operation that makes it an efficient operation that has a fast response. Let's have a look on example *pi.py*: We first call few libraries as below. </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"margin: 1px\">\n",
    "<p>\n",
    "import sys <br>\n",
    "from random import random <br>\n",
    "from operator import add <br>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> As we are aware of requirement of using SparkContext linrary too, which is also needs to import. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"margin: 1px\">\n",
    "<p>\n",
    "from pyspark import SparkContext\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> Now, in the main function, we will perform the operation for calulating pi as followed. Here we are using the system argument to calculate the pi value </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"margin: 1px\">\n",
    "<p>\n",
    "sc = SparkContext(appName = \"PythonPi\")<br>\n",
    "partitions = int(sys.argv[1]) if len(sys.argv) > 1 else 2<br>\n",
    "n = 100000 * partitions<br>\n",
    "def(f):<br>\n",
    "    <span style=\"margin-left:2em\">x = random()* 2 - 1 </span><br>\n",
    "    <span style=\"margin-left:2em\">y = random()* 2 - 1 </span><br>\n",
    "    <span style=\"margin-left:2em\">return 1 if x ** 2 + y ** 2 < 1 else 0 </span><br>\n",
    "  \n",
    "count = sc.parallelize(range(1, n+1), partitions).map(f).reduce(add)<br>\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / n))<br>\n",
    "sc.stop<br>\n",
    "</p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> combining everything togather into the executable code: </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!spark-2.0.1-bin-hadoop2.7/./bin/spark-submit /home/cc/spark-2.0.1-bin-hadoop2.7/pi.py 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Word Count Operation of a Text File** </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> Using all the concepts of RDDs we learned in the previous lesson, we will move to implement an example of word count for an existing file inside the spark folder we have saved it as \"CHANGES.txt\" </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> The main purpose of this progrmam is to calculate the wordcount of the given file, in this case it is text. This would be a good usecase when we are dealing with some of the big chunk file & need some set of things to be calculated. </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!spark-2.0.1-bin-hadoop2.7/./bin/spark-submit /home/cc/spark-2.0.1-bin-hadoop2.7/wordcount.py spark-2.0.1-bin-hadoop2.7/CHANGES.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Big Data Analysis: Airlines Delay** </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> In order to demonstrate a common use of Spark, let's take a look at a common use case where we read in a CSV file of data and compute some aggregate statistic. In this case, we're looking at the on-time flight data set from the U.S. Department of Transportation, recording all U.S. domestic flight departure and arrival times along with their departure and arrival delays for the month of April, 2014. I typically use this data set because one month is manageable for exploration, but the entire data set needs to be computed upon with a cluster.  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> So what is this code doing?\n",
    "Let's look particularly at the main function which does the work most directly related to Spark. First, we load up a **CSV file** into an RDD, then map the split function to it. The split function parses each line of text using the csv module and returns a tuple that represents the row. Finally we pass the collect action to the RDD, which brings the data from the RDD back to the driver as a Python list. In this case, **airlines.csv** is a small jump table that will allow us to join airline codes with the airline full name. We will store this jump table as a Python dictionary and then broadcast it to the node  sc.broadcast, which also works for the case when we have a cluster of Spark. \n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> Next, the main function loads the much larger **flights.csv**. After splitting the CSV rows, we map the parse function to the CSV row, which converts dates and times to Python dates and times, and casts floating point numbers appropriately. It also stores the row as a NamedTuple called Flight for efficient ease of use. </span>\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\">With an RDD of Flight objects in hand, we map an anonymous function that transforms the RDD to a series of key-value pairs where the key is the name of the airline and the value is the sum of the arrival and departure delays. Each airline has its delay summed together using the reduceByKey action and the add operator, and this RDD is collected back to the driver (again the number airlines in the data is relatively small). Finally the delays are sorted in ascending order, then the output is printed to the console as well as visualized using matplotlib. </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> To run this code, use the spark-submit command as follows:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!spark-2.0.1-bin-hadoop2.7/./bin/spark-submit /home/cc/spark-2.0.1-bin-hadoop2.7/flightdelay.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> This will generathe the below figure: </span>\n",
    "<div style=\"width:830; background-color:white; height:540px; overflow:scroll; overflow-x: scroll;overflow-y: hidden;\">\n",
    "\n",
    "<img style=\" float:left; display:inline\" src=\"\" width=\"160\" height=\"70\"/>\n",
    "\n",
    "<img style=\" float:left; display:inline\" src=\"http://129.114.111.241:8888/tree/spark-2.0.1-bin-hadoop2.7/delays.png\" width=\"860\" height=\"420\"/> </div>\n",
    "\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\">This will create a Spark job using the localhost as the master, and look for the two **CSV** files in an ontime directory that is in the same directory as the python code is in. The final result shows that the total delays (in minutes) for the perticlar month. on our case we took the data for the month of April that goes from arriving early if you're flying out of the continental U.S. to Hawaii or Alaska to an aggregate total delay for most big airlines. Note especially that we can visualize the result using matplotlib directly on the driver program, with the above python code: </span>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
