{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px;border:none;color:#333;background-color:#333;\" />\n",
    "<img style=\" float:right; display:inline\" src=\"http://opencloud.utsa.edu/wp-content/themes/utsa-oci/images/logo.png\"/>\n",
    "\n",
    "### **University of Texas at San Antonio** \n",
    "<br/>\n",
    "<br/>\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 2.5em;\"> **Open Cloud Institute** </span>\n",
    "\n",
    "<hr style=\"height:3px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson 1: Big Data Analysis with Spark\n",
    "\n",
    "<br/>  \n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Nimish Joshi, Research Fellow** </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.4em;\"> *Open Cloud Institute, University of Texas at San Antonio, San Antonio, Texas, USA* </span>  \n",
    "<hr style=\"height:3px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Objectives** </span> <span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> Apache Spark is a lightning-fast cluster computing designed for fast computation. It was built on top of Hadoop MapReduce and it extends the MapReduce model to efficiently use more types of computations which includes Interactive Queries and Stream Processing. This is a brief tutorial that explains the basics of Spark Core programming.\n",
    "</span> \n",
    "\n",
    "[1]: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-8/faceimages/faces/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Prerequisites** </span> <span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> Before you start proceeding with this tutorial, we assume that you have prior exposure to:\n",
    "* Python Programming\n",
    "* Scripting language experience\n",
    "* Linux OS flavor</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Introduction** </span> <span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> Spark was introduced by Apache Software Foundation for speeding up the Hadoop computational computing software process. As against a common belief, Spark is not a modified version of Hadoop and is not, really, dependent on Hadoop because it has its own cluster management. Hadoop is just one of the ways to implement Spark.\n",
    "</span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\">  **Evolution of Apache Spark** Spark is one of Hadoop’s sub project developed in 2009 in UC Berkeley’s AMPLab by Matei Zaharia. It was Open Sourced in 2010 under a BSD license. It was donated to Apache software foundation in 2013, and now Apache Spark has become a top level Apache project from Feb-2014.</span>\n",
    "\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\">  **Features of Apache Spark** Apache Spark has following features.\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> \n",
    "* **Speed** Spark helps to run an application in Hadoop cluster, up to 100 times faster in memory, and 10 times faster when running on disk. This is possible by reducing number of read/write operations to disk. It stores the intermediate processing data in memory.\n",
    "* **Supports multiple languages** Spark provides built-in APIs in Java, Scala, or Python. Therefore, you can write applications in different languages. Spark comes up with 80 high-level operators for interactive querying.\n",
    "* **Advanced Analytics:** Spark not only supports ‘Map’ and ‘reduce’. It also supports SQL queries, Streaming data, Machine learning (ML), and Graph algorithms.\n",
    "</span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Spark Built on Hadoop** </span> <span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> The following shows three ways how spark can be build up in Hadoop components.\n",
    "</span>\n",
    "\n",
    "<div style=\"width:830; background-color:white; height:310px; overflow:scroll; overflow-x: scroll;overflow-y: hidden;\">\n",
    "\n",
    "<img style=\" float:left; display:inline\" src=\"\" width=\"160\" height=\"90\"/>\n",
    "\n",
    "<img style=\" float:left; display:inline\" src=\"http://129.114.111.241:8888/tree/spark-2.0.1-bin-hadoop2.7/spark_built.png\" width=\"560\" height=\"380\"/>\n",
    "\n",
    "</div>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> There are three ways of Spark deployment as explained bewlow.\n",
    "</span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> \n",
    "* **Standalone** Spark Standalone deployment means Spark occupies the place on top of HDFS(Hadoop Distributed File System) and space is allocated for HDFS, explicitly. Here, Spark and MapReduce will run side by side to cover all spark jobs on cluster.\n",
    "* **Hadoop Yarn:** Hadoop Yarn deployment means, simply, spark runs on Yarn without any pre-installation or root access required. It helps to integrate Spark into Hadoop ecosystem or Hadoop stack. It allows other components to run on top of stack.\n",
    "* **Spark in MapReduce (SIMR):**Spark in MapReduce is used to launch spark job in addition to standalone deployment. With SIMR, user can start Spark and uses its shell without any administrative access.\n",
    "</span>\n",
    "\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\">  **Components of Spark**  The following illustration depicts the different components of Spark.\n",
    "</span>\n",
    "\n",
    "<div style=\"width:830; background-color:white; height:280px; overflow:scroll; overflow-x: scroll;overflow-y: hidden;\">\n",
    "\n",
    "<img style=\" float:left; display:inline\" src=\"\" width=\"160\" height=\"90\"/>\n",
    "\n",
    "<img style=\" float:left; display:inline\" src=\"http://129.114.111.241:8888/tree/spark-2.0.1-bin-hadoop2.7/Spark_Components.png\" width=\"560\" height=\"380\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\">  **Apache Spark Core** Spark Core is the underlying general execution engine for spark platform that all other functionality is built upon. It provides In-Memory computing and referencing datasets in external storage systems.</span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\">  **Spark SQL** Spark SQL is a component on top of Spark Core that introduces a new data abstraction called SchemaRDD, which provides support for structured and semi-structured data.</span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\">  **Spark Streaming** Spark Streaming leverages Spark Core's fast scheduling capability to perform streaming analytics. It ingests data in mini-batches and performs RDD (Resilient Distributed Datasets) transformations on those mini-batches of data.</span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\">  **MLlib (Machine learning Library of spark)** MLlib is a distributed machine learning framework above Spark because of the distributed memory-based Spark architecture. It is, according to benchmarks, done by the MLlib developers against the Alternating Least Squares (ALS) implementations. Spark MLlib is nine times as fast as the Hadoop disk-based version of Apache Mahout (before Mahout gained a Spark interface).</span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\">  **GraphX** GraphX is a distributed graph-processing framework on top of Spark. It provides an API for expressing graph computation that can model the user-defined graphs by using Pregel abstraction API. It also provides an optimized runtime for this abstraction.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.5em;\"> **Spark Streaming** </span>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams. </span>\n",
    "\n",
    "<div style=\"width:830; background-color:white; height:240px; overflow:scroll; overflow-x: scroll;overflow-y: hidden;\">\n",
    "\n",
    "<img style=\" float:left; display:inline\" src=\"\" width=\"160\" height=\"70\"/>\n",
    "\n",
    "<img style=\" float:left; display:inline\" src=\"http://129.114.111.241:8888/tree/spark-2.0.1-bin-hadoop2.7/streaming.png\" width=\"660\" height=\"420\"/> </div>\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> Internally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches. </span>\n",
    " \n",
    "<div style=\"width:830; background-color:white; height:200px; overflow:scroll; overflow-x: scroll;overflow-y: hidden;\">\n",
    "\n",
    "<img style=\" float:left; display:inline\" src=\"\" width=\"160\" height=\"90\"/>\n",
    "\n",
    "<img style=\" float:left; display:inline\" src=\"http://129.114.111.241:8888/tree/spark-2.0.1-bin-hadoop2.7/streaming2.png\" width=\"660\" height=\"480\"/> </div>\n",
    "\n",
    "\n",
    "<span style=\"color:#000; font-family: 'Bebas Neue'; font-size: 1.3em;\"> Spark Streaming provides a high-level abstraction called discretized stream or DStream, which represents a continuous stream of data. DStreams can be created either from input data streams from sources such as Kafka, Flume, and Kinesis, or by applying high-level operations on other DStreams. Internally, a DStream is represented as a sequence of RDDs. </span>\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
